{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Probability\n",
    "\n",
    "## Associated Reading: Bishop 1.1, 1.2, 2.1\n",
    "\n",
    "## 1. A definition of probability\n",
    "We live in an uncertain world.  Sometimes, as computer scientists, we're sheltered from that because we think about things like deterministic algorithms for which there are certain guarantees about, for example, the amount of time it will take to run, or perhaps even more basic, that the procedure will yield a correct result every time.  For a given input, the path to a given output is well-defined (suspending consideration of the very interesting class of randomized algorithms).  However, when we do Machine Learning, we're doing something a bit different: we don't know how to write the program, so we're going to try and reconstruct it by observing the world$^1$.  In so doing, we have to come to terms with the notion that those observations are going to be imperfect in a variety of ways: imperfection in our means of observing, systematic errors in the way that we represent such information, wrong assumptions about how we allow our learning machines to behave, and old-fashioned, honest-to-goodness randomness.  Ignoring these sources of imperfection does not work: it is far better to be conscious of our uncertainties than overconfident in a notion that doesn't actually capture the truth.  The practical implications of this is that we are going to need to become comfortable expressing the lessons that we learn from data, and the predictions that we make based on what we've learned, in a language that's suited towards expressing these uncertainties.  That language is probability.   \n",
    "\n",
    "The stuff that we need to start working with the language of probability in a practical way is less extensive than you are probably expecting.  All that we need to take us quite a ways is a definition and a few rules for manipulation.  Let's start with a definition, and there's actually some competition between a few different ideas here.  You would think that with the ubiquity of statistics in modern society, that this would have already been settled 200 years ago.  But no, the debate continues to this day.  The two candidate definitions are:\n",
    "- (Frequentist) $P(X=x)$ is the number of times we observe that a random variable $X=x$ for an infinite number of trials, divided by the total number of trials.  For example, in the limit as $n\\rightarrow \\infty$, the probability of a fair coin landing on heads is 1/2, so $P(coin=heads)=0.5$.  \n",
    "$$ \\frac{N_{X=x}}{N_{trials}}, N_{trials}\\rightarrow \\infty $$\n",
    "- (Bayesian) $P(X=x)$ is the degree to which we believe that for a given trial, we will observe that $X=x$.  So for $P(X=x)=1$, we're certain that $X=x$, and for $P(X=x)=0$, we're certain that this is not the case, and $P(X=x) = 0.5$, it's fifty-fifty.    \n",
    "\n",
    "The difference probably seems totally pedantic to you right now, but it actually isn't, especially in the context of machine learning or inverse problems in general.  In the case of the coin flip example, there's really not that much difference, because we can perform many repeats of the same experiment, such that we can approximate the limit as the number of trials go to infinity, and use those trials to update our probabilistic model of the coin.  However, there's a meaningful distinction when we start to reason about things that we can't conduct experiments on.  My research application area is in predicting change in the polar ice caps, and it provides a nice example: let's say that $\\mathcal{H}$ is the specific hypothesis that 200 years from now, the Greenland Ice Sheet will have completely melted.  Intuitively, it's reasonable to ask what's $P(\\mathcal{H})$?  What's the probability that this hypothesis is true?  \n",
    "\n",
    "If we plug this specific case into the frequentist definition, we see the problem: $P(H)$ is the number of times that we observe the Greenland Ice Sheet to melt out of an infinite number of trials?  We don't have an infinite number of trials to work with: we only have the one.  In contrast, there's no particular problem with the Bayesian definition: it's simply a statement that $P(H)$ encodes our current state of information about Greenland's future, or more generally $P(H)$ is a formal quantification of the information that we have about a preposition, when we're not yet exactly sure whether the preposition is true or false.  In fact, this idea is just a generalization of something that you already know about, which is prepositional logic: probability is the extension of logic to situations where prepositions aren't either one or zero, but something in the middle.  If it's not already clear, this second so-called Bayesian definition of probability is the one that we're going to concern ourselves with.  For practical purposes, we need to know just a few identities that largely mirror prepositional logic: the product rule, which is akin to the AND operation, and the sum rule, which is akin to OR.    \n",
    "\n",
    "$^1$. (This analogy is pretty loose, because we're actually going to exert quite a bit of control on exactly what types of programs we \"learn\" by specifying a mathematical model, although some cutting edge work towards general AI is indeed oriented towards trying to synthesize syntactically valid programs from data). \n",
    "\n",
    "## 2. Joint probability rules\n",
    "<img src=\"images/grid_fig.jpg\" style=\"width: 500px;\">\n",
    "\n",
    "These two rules are best understood in the context of joint probability distributions, or the probability of two events occurring at the same time.  For example, $P(X=x,Y=y)$ quantifies our belief that $X=x$ and $Y=y$$^2$.\n",
    "\n",
    "This is most easily understood by drawing a grid in two dimensions.  On the horizontal axis is the random variable X, on the vertical axis is the random variable Y.  Then we divide the axes into three subdivisions each, and call each subdivision 1,2, or 3.  \n",
    "\n",
    "$^2$ (If you haven't seen this upper case-lower case notation before, here's how it works.  Upper case letters are random variables: it's just the name of a thing that could take different values.  It's been declared, and it has a type, but we don't know the exact value.  Lower case letters represent actual specific values.  For example if $X$ is a random variable representing the outcome of a coin flip, then the *support* of $X$ is the set $\\{heads,tails\\}$.  $x$ in this case is a specific outcome from that set, i.e. for a fair coin $P(X=heads) = 50$%.)      \n",
    "\n",
    "### 2.1 Sum rule\n",
    "So imagine, that this thing is a set of boxes, and we're randomly throwing balls into the boxes.  The probability of the ball landing in any particular box is given by the joint probability $P(X,Y)$.  In our case, we have 9 cells, so $P(X=2,Y=2) = 1/9$.  \n",
    "\n",
    "**Now, what is $P(X=2)$?**\n",
    "\n",
    "It is, of course, $1/3$.  This is pretty obvious, but it's instructive to write it as \n",
    "$$P(X=2) = P(X=2,Y=1) + P(X=2,Y=2) + P(X=2,Y=3) = \\sum_{i=1}^3 P(X=2,Y=y_i) = 1/3.$$\n",
    "$P(X=2)$ is known as the *marginal probability*, and this procedure is known as the sum rule, or generally:\n",
    "$$\n",
    "P(X=x_j) = \\sum_{i=1}^m P(X=x_j,Y=y_i).  \n",
    "$$\n",
    "\n",
    "### 2.2 Product rule\n",
    "Now let's consider cases where we're given the value of one of the random variables.  This is called *conditional probability*, annotated $P(Y=y_j|X=x_i)$.  \n",
    "\n",
    "**If we know that $X=2$, what is the probability that $Y=2$?**  This is pretty obvious graphically, but worth writing down the actual mathematical relationship between the joint distribution and the conditional, which is \n",
    "$$P(X=x,Y=y) = P(X=x|Y=y) P(Y=y)$$.  We are after the first term on the right side, so if we divide the left hand term (which is 1/9) by the rightmost (which is 1/3), we get, as expected, 1/3.  \n",
    "\n",
    "### 2.3 Exchangability \n",
    "Nothing changes if we swap the axis-labels, so $P(X=x,Y=y) = P(Y=y,X=x)$.\n",
    "\n",
    "### 2f.4 Conditional probability examples\n",
    "Equipped with these probability rules, now let's use them to answer some simple questions.  To begin, Let's say that we've got two two bowls, green and blue.  Inside green, we have 1 kiwis and 3 oranges.  Inside blue, we have 3 kiwis and 1 orange.  Now what if I asked the question, whats the probability of drawing an orange if I sample from both bowls with equal likelihood?  Then we have that\n",
    "\\begin{align}\n",
    "P(Orange) & = P(Orange,Blue) + P(Orange,Green) \\\\\n",
    "          & = P(Orange|Blue)P(Blue) + P(Orange|Green)P(Green) \\nonumber \\\\\n",
    "        & = (1/4)(1/2) + (3/4)(1/2) = 1/2. \\nonumber\n",
    "\\end{align}\n",
    "*Note that I've simplified notation a little bit: Instead of writing P(Fruit=Orange,Bowl=Blue), I'm just writing P(Orange,Blue)*.  What if I move these three kiwis out of blue, and into green.  So now there's 1 orange in green, and 4 kiwis and three oranges in blue.  What's the probability of an orange?\n",
    "\\begin{align}\n",
    "P(Orange) & = P(Orange,Blue) + P(Orange,Green) = P(Orange|Blue)P(Blue) \\nonumber \\\\\n",
    "          & + P(Orange|Green)P(Green) \\nonumber \\\\\n",
    "          & = (3/7)(1/2) + (1)(1/2) = 3/14 + 7/14 = 5/7\n",
    "\\end{align}\n",
    "We are now more likely to pick a bowl with more oranges, so the overall likelihood of picking an orange goes up.  Now, what if I ask, under the same rules as above, \"I picked an orange.  What is the probability that this orange came from the green bowl?\".  This requires a bit more thought.  Before, we had a scenario where our conditional probabilities were determined entirely by things that we knew already.  Given a bowl color, I can tell you directly what the probability of a fruit is.  But in this case, oranges could come from either bowl, so having an orange doesn't directly determine the probability of a bowl.  Instead we need to come up with a rule for inverting probability in some way.  And this isn't very difficult using the tools we have above.  Let's start with the identity:\n",
    "$$P(X=x,Y=y) = P(Y=y,X=x),$$\n",
    "now apply the product rule and divide:\n",
    "$$P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}$$\n",
    "Now we can use this result directly:\n",
    "$$P(Green|Orange) = \\frac{P(Orange|Green)P(Green)}{P(Orange)} = (1)(1/2)(7/5) = 7/10$$\n",
    "Because green was a sure bet for orange, and the probability of choosing green was equal to that of blue, we find that it's quite a bit more likely that the orange came from the green bowl.\n",
    "\n",
    "### 2.5 Bayes rule\n",
    "The formula that we used to compute this turns out to be so important that it has its own special proper-noun name: Bayes' Theorem.  The name is stupid by the way, it was in fact Pierre-Simon Laplace who first understood it in its modern usage and also developed some of its philosophical implications.  And once again, you're probably thinking \"so what?\", how does this help me do machine learning?  And the answer becomes a little bit more plain if we apply Bayes' theorem to the case where we have to pick between competing hypotheses that describe the world, which we'll call models $M$ given some real data about the world, which we'll call $D$.  Bayes' theorem then says \n",
    "$$P(M=m|D=d) = \\frac{P(D=d|M=m)P(M=m)}{P(D=d)},$$\n",
    "which is to say that it provides us a statistical formula for selecting between models of reality given observations of said reality.  Let's take a look at the anatomy of this equation:  The thing on the left is called the *posterior probability* or the probability that a given hypothesis is true after considering the data.  On the right hand side, $P(M=m)$ is called the *prior probability*, which is the probability of a particular hypothesis being the correct one *prior* to considering the data.  The prior probability is updated according to the first term in the numerator which is called the *likelihood*, which answers the question, assuming that a particular model $M=m$, what is the probability that we will observe the data $y$?  Finally, the denominator is often called the *evidence*, and it's the probability of observing the data under all possible hypotheses.  \n",
    "\n",
    "This formula shapes your life and your thinking in ways that you have never imagined.  Least squares linear regression (which we'll get to)? Bayes rule.  Want to go bet on a horse race and be sure that you're not being scammed?  Bayes' rule.  *The fundamental human process of updating actions based on sensory input*?  Empirically, also Bayes' rule.  Everything (and I mean everything) that we do in this course from here on will be some application of Bayes rule, sometimes explicitly, sometimes implicitly.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09/02\n",
    "*From your group, choose a scribe.  Then compose a document responding to the following.  When complete, please turn in the assignment to the appropriate link in this week's Moodle page.*\n",
    "\n",
    "A classic example of Bayes' theorem in action comes from the world of disease testing. This is a really good example because it shows the importance of the prior distribution. So here's the problem: imagine that you're concerned that you have COVID-19, something that currently affects roughly 7 in 1000 people in the US, and you go down to the county health services to take a COVID test. The doctor tells you that the sensitivity of the test is 70%, which is to say that if you have the disease the test will read positive 70% of the time. We also know that the test has a specificity of 98%, which implies that 98% of the time, if you don't have the disease, the test will read negative. Unfortunately, your test comes back positive. \n",
    "\n",
    "**Compute the probability that you actually have the disease.**\n",
    "\n",
    "**Using python (matplotlib), generate a plot showing the probability above as a function of the specificity.**\n",
    "\n",
    "**Devise a simple strategy for reducing the false positive rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
